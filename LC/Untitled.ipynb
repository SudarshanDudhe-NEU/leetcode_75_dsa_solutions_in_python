{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466eb273-28e1-449d-8c9c-5fb1fd7ab386",
   "metadata": {},
   "source": [
    "CSYE7105 Assignment\n",
    "Part 1: 34 Points\n",
    "Question 1: Difference Between CPU and GPU [5 Points]\n",
    "Central Processing Unit (CPU):\n",
    "\n",
    "Purpose: General-purpose processor designed to handle a wide variety of tasks.\n",
    "Architecture:\n",
    "Few Powerful Cores: Typically 2 to 16 cores in consumer CPUs.\n",
    "High Clock Speed: Individual cores run at high frequencies (~3-5 GHz).\n",
    "Processing:\n",
    "Sequential Processing: Optimized for tasks that require complex computations and can't be easily parallelized.\n",
    "Low Latency: Quick response times for individual tasks.\n",
    "Memory Hierarchy:\n",
    "Large Cache Memory: Multiple levels (L1, L2, L3) to reduce latency.\n",
    "Use Cases:\n",
    "General Computing: Running operating systems, applications, and performing tasks that require complex logic.\n",
    "Graphics Processing Unit (GPU):\n",
    "\n",
    "Purpose: Specialized processor designed for parallel processing, primarily for rendering graphics.\n",
    "Architecture:\n",
    "Thousands of Smaller Cores: Designed for handling multiple tasks simultaneously.\n",
    "Lower Clock Speed: Cores run at lower frequencies (~1-2 GHz).\n",
    "Processing:\n",
    "Parallel Processing: Optimized for tasks that can be divided into smaller, similar subtasks.\n",
    "High Throughput: Maximizing the amount of processed data over time.\n",
    "Memory Hierarchy:\n",
    "High Bandwidth Memory: Optimized for handling large data sets.\n",
    "Use Cases:\n",
    "Graphics Rendering: 3D rendering, image processing.\n",
    "Compute-Intensive Tasks: Machine learning, scientific simulations.\n",
    "Key Differences:\n",
    "\n",
    "Processing Style: CPUs excel at sequential tasks with complex logic, while GPUs excel at parallel tasks with simple computations.\n",
    "Core Count and Power: CPUs have fewer, more powerful cores; GPUs have many, less powerful cores.\n",
    "Performance Goals: CPUs aim for low latency; GPUs aim for high throughput.\n",
    "Application Domains: CPUs handle general computing tasks; GPUs handle tasks requiring massive parallelism.\n",
    "Question 2: GPU Memory Hierarchy [5 Points]\n",
    "Understanding the GPU memory hierarchy is essential for optimizing performance in GPU computing. The hierarchy includes several types of memory, each with different characteristics:\n",
    "\n",
    "Registers:\n",
    "\n",
    "Location: Within each GPU core.\n",
    "Size: Very small (per-thread).\n",
    "Speed: Fastest memory available to threads.\n",
    "Usage: Storing variables and data needed immediately by threads.\n",
    "Shared Memory:\n",
    "\n",
    "Location: On-chip memory within each Streaming Multiprocessor (SM).\n",
    "Size: Limited (typically 64 KB per SM).\n",
    "Speed: Nearly as fast as registers.\n",
    "Usage: Shared among threads in the same thread block for efficient communication.\n",
    "L1 Cache:\n",
    "\n",
    "Location: On-chip within each SM.\n",
    "Size: Small (tens of KB).\n",
    "Speed: Very fast.\n",
    "Usage: Caches local data to reduce access to slower global memory.\n",
    "L2 Cache:\n",
    "\n",
    "Location: Shared across all SMs.\n",
    "Size: Larger than L1 (MB range).\n",
    "Speed: Slower than L1 but faster than global memory.\n",
    "Usage: Reduces global memory access latency for all SMs.\n",
    "Global Memory:\n",
    "\n",
    "Location: Off-chip DRAM.\n",
    "Size: Large (several GBs).\n",
    "Speed: High latency compared to on-chip memory.\n",
    "Usage: Main memory for the GPU, accessible by all threads.\n",
    "Constant Memory:\n",
    "\n",
    "Location: Portion of global memory with cache.\n",
    "Size: Small (up to 64 KB).\n",
    "Speed: Cached for fast reads.\n",
    "Usage: Storing read-only data that is constant across threads.\n",
    "Texture Memory:\n",
    "\n",
    "Location: Portion of global memory with specialized cache.\n",
    "Size: Depends on GPU.\n",
    "Speed: Optimized for spatial locality.\n",
    "Usage: Handling 2D spatial data, beneficial for certain algorithms.\n",
    "Local Memory:\n",
    "\n",
    "Location: Off-chip global memory.\n",
    "Size: Depends on usage.\n",
    "Speed: Same latency as global memory.\n",
    "Usage: Used when registers are insufficient for a thread's data.\n",
    "Key Points:\n",
    "\n",
    "Latency and Bandwidth: On-chip memories (registers, shared memory) have low latency and high bandwidth. Off-chip memories (global memory) have higher latency.\n",
    "Optimizing Performance: Effective GPU programming minimizes global memory accesses and maximizes the use of shared memory and registers.\n",
    "Memory Access Patterns: Coalesced memory accesses and proper use of caches improve performance.\n",
    "Question 3: Tasks on CPU Node [10 Points]\n",
    "Step-by-Step Instructions:\n",
    "\n",
    "Request a CPU Node on Discovery:\n",
    "\n",
    "Open a terminal and use the Slurm scheduler to request an interactive session:\n",
    "bash\n",
    "Copy code\n",
    "srun --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=4 --time=01:00:00 --pty /bin/bash\n",
    "Activate Your Own Conda Environment:\n",
    "\n",
    "Load the Anaconda module if not already loaded:\n",
    "bash\n",
    "Copy code\n",
    "module load anaconda3/2020.11\n",
    "Create a new Conda environment (if you haven't already):\n",
    "bash\n",
    "Copy code\n",
    "conda create -n myenv python=3.8\n",
    "Activate your Conda environment:\n",
    "bash\n",
    "Copy code\n",
    "conda activate myenv\n",
    "Enter Python Interactive Mode and Check PyTorch Version:\n",
    "\n",
    "Install PyTorch in your environment if necessary:\n",
    "bash\n",
    "Copy code\n",
    "conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "Start Python interactive mode:\n",
    "bash\n",
    "Copy code\n",
    "python\n",
    "Import PyTorch and check the version:\n",
    "python\n",
    "Copy code\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "Exit Python:\n",
    "python\n",
    "Copy code\n",
    "exit()\n",
    "Deactivate Your Conda Environment:\n",
    "\n",
    "Deactivate the environment:\n",
    "bash\n",
    "Copy code\n",
    "conda deactivate\n",
    "Screenshot Instructions:\n",
    "\n",
    "Take a screenshot showing the terminal with the commands executed and the output, including the PyTorch version.\n",
    "Question 4: Tasks on GPU Node [14 Points]\n",
    "Step-by-Step Instructions:\n",
    "\n",
    "Request a GPU Node on Discovery:\n",
    "\n",
    "Request an interactive GPU node:\n",
    "bash\n",
    "Copy code\n",
    "srun --partition=gpu --gres=gpu:1 --nodes=1 --ntasks=1 --cpus-per-task=4 --time=01:00:00 --pty /bin/bash\n",
    "Load CUDA Module [1 Point]:\n",
    "\n",
    "Load CUDA 11.x module:\n",
    "bash\n",
    "Copy code\n",
    "module load cuda/11.3\n",
    "Use nvidia-smi for GPU Information [8 Points]:\n",
    "\n",
    "Static Information:\n",
    "\n",
    "Run:\n",
    "bash\n",
    "Copy code\n",
    "nvidia-smi\n",
    "Explanation:\n",
    "Displays driver version, CUDA version, GPU name, total and used memory.\n",
    "Shows the overall status of the GPU at the moment.\n",
    "Dynamic Information:\n",
    "\n",
    "Run:\n",
    "bash\n",
    "Copy code\n",
    "nvidia-smi -l 1\n",
    "Explanation:\n",
    "Updates the GPU information every second.\n",
    "Useful for monitoring real-time GPU usage while running applications.\n",
    "GPU Memory and Hierarchy Explanation:\n",
    "\n",
    "Global Memory:\n",
    "The \"FB Memory Usage\" section shows the Frame Buffer (device memory) usage.\n",
    "Indicates how much memory is allocated and available.\n",
    "GPU Utilization:\n",
    "The \"Utilization\" section shows how busy the GPU is.\n",
    "Helps identify if the GPU is being effectively utilized.\n",
    "Temperature and Power:\n",
    "Monitoring these ensures the GPU is operating within safe limits.\n",
    "Screenshots:\n",
    "\n",
    "Take two screenshots:\n",
    "Static nvidia-smi output.\n",
    "Dynamic nvidia-smi output while an application is running (if possible).\n",
    "Use PyTorch to Check GPU Availability [5 Points]:\n",
    "\n",
    "Activate Conda Environment:\n",
    "bash\n",
    "Copy code\n",
    "conda activate myenv\n",
    "Install PyTorch with CUDA Support:\n",
    "bash\n",
    "Copy code\n",
    "conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n",
    "Start Python Interactive Mode:\n",
    "bash\n",
    "Copy code\n",
    "python\n",
    "Check if GPU is Available [2 Points]:\n",
    "python\n",
    "Copy code\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "Get Current GPU Device ID [2 Points]:\n",
    "python\n",
    "Copy code\n",
    "current_device = torch.cuda.current_device()\n",
    "print(\"Current CUDA Device ID:\", current_device)\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(current_device))\n",
    "Screenshot [1 Point]:\n",
    "Capture the Python session showing the above commands and their outputs.\n",
    "Exit Python and Deactivate Conda Environment:\n",
    "python\n",
    "Copy code\n",
    "exit()\n",
    "conda deactivate\n",
    "Part 2: 66 Points\n",
    "Implementing DDP on Multiple CPUs and Analyzing Speedup Performance\n",
    "Objective:\n",
    "\n",
    "Modify the existing multi-GPU parallel code to implement Distributed Data Parallel (DDP) on multiple CPUs and analyze the calculation speedup performance.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Access the Code File:\n",
    "\n",
    "Copy CSYE7105-pyt05.ipynb from /courses/CSYE7105.202510/shared/week11 to your working directory.\n",
    "Understanding the Existing Code:\n",
    "\n",
    "The current code utilizes multiple GPUs for parallel processing.\n",
    "It likely uses PyTorch's DataParallel or DDP for GPUs.\n",
    "Modify the Code for CPU DDP:\n",
    "\n",
    "Import Necessary Libraries:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "Initialize the Process Group:\n",
    "\n",
    "Use the gloo backend for CPU.\n",
    "python\n",
    "Copy code\n",
    "def setup(rank, world_size):\n",
    "    dist.init_process_group(\n",
    "        backend='gloo',\n",
    "        init_method='tcp://127.0.0.1:29500',\n",
    "        rank=rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "Define the Model and Wrap with DDP:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    # Model initialization\n",
    "    model = Net()\n",
    "    ddp_model = DDP(model)\n",
    "    # Rest of the training code\n",
    "Adjust the DataLoader with DistributedSampler:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "train_dataset = datasets.MNIST(...)\n",
    "train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "Modify the Main Function to Spawn Processes:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def main():\n",
    "    world_size = 4  # Number of CPUs\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "Running the Modified Code:\n",
    "\n",
    "Request Resources on Discovery:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "srun --partition=compute --nodes=1 --ntasks=4 --cpus-per-task=1 --time=02:00:00 --pty /bin/bash\n",
    "Load Necessary Modules and Activate Environment:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "module load anaconda3/2020.11\n",
    "conda activate myenv\n",
    "Run the Jupyter Notebook via OOD:\n",
    "\n",
    "Start a Jupyter session on the allocated node.\n",
    "Run the notebook and ensure all processes start correctly.\n",
    "Analyzing Calculation Speedup Performance:\n",
    "\n",
    "Measure Training Time:\n",
    "\n",
    "Record the total training time using multiple CPUs.\n",
    "Also, run the training on a single CPU and record the time.\n",
    "Calculate Speedup:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "speedup = single_cpu_time / multi_cpu_time\n",
    "Plotting the Results:\n",
    "\n",
    "Create a plot with the number of CPUs on the x-axis and training time on the y-axis.\n",
    "Discuss the scaling behavior.\n",
    "Documenting Node Information:\n",
    "\n",
    "Add Node Details in Markdown:\n",
    "\n",
    "At the beginning or end of the notebook, include:\n",
    "Node Name: Use !hostname in a code cell.\n",
    "Number of CPUs Used: Mention world_size or the number of tasks.\n",
    "Hardware Specifications: Any relevant details.\n",
    "Example:\n",
    "\n",
    "markdown\n",
    "Copy code\n",
    "### Node Information\n",
    "\n",
    "- **Node Name**: compute-node-01\n",
    "- **Number of CPUs Used**: 4\n",
    "- **Hardware Specs**: Intel Xeon CPU E5-2680 v4 @ 2.40GHz\n",
    "Discussion:\n",
    "\n",
    "Analyze Results:\n",
    "\n",
    "Comment on the observed speedup.\n",
    "Discuss any overheads or inefficiencies.\n",
    "Explain whether the scaling is linear and why it may not be.\n",
    "Possible Issues:\n",
    "\n",
    "Communication Overhead: With more CPUs, the communication between processes may introduce overhead.\n",
    "Data Loading Bottlenecks: Ensure data loading is efficient and not a bottleneck.\n",
    "Finalizing the Notebook:\n",
    "\n",
    "Ensure all cells have been run and outputs are saved.\n",
    "Check for any errors or warnings.\n",
    "Save the notebook with all outputs visible.\n",
    "Submission:\n",
    "\n",
    "Submit the completed Jupyter notebook as per the assignment guidelines.\n",
    "Ensure the node information and analysis are included.\n",
    "End of Assignment\n",
    "Note: The above steps provide detailed instructions for completing the assignment tasks. Ensure that you follow each step carefully and include the required screenshots and explanations in your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72cd49e-4629-4e7a-9db1-1f3624d887aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
